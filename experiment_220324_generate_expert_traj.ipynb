{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e1d2eb7-5890-4575-b97d-44f1c0467bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bombergym.scenarios import classic_with_opponents\n",
    "from bombergym.environments import register\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aee7309-be1f-4038-89bd-0d7f0f607e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "register()\n",
    "settings, agents = classic_with_opponents()\n",
    "env = gym.make('BomberGym-v4', args=settings, agents=agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0babab8f-275c-46e4-ac5e-92f386813113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "state is none\n",
      "actions (22173, 1)\n",
      "obs (22173, 5, 17, 17)\n",
      "rewards (22173,)\n",
      "episode_returns (100,)\n",
      "episode_starts (22173,)\n"
     ]
    }
   ],
   "source": [
    "dict = generate_expert_traj(expert, save_path=\"test-experttraj\", env=env, n_timesteps=10000, n_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff2f4a8c-5f89-43f6-889b-e1ca7669f785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32418dfb-e25f-4d3b-9356-01b25c990c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from bombergym.agent_code.rule_based_agent.callbacks import act, setup\n",
    "from bombergym.settings import ACTIONS\n",
    "\n",
    "class Self:\n",
    "    logger = logging.getLogger(\"Self\")\n",
    "\n",
    "self=Self()\n",
    "setup(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2879056f-da86-4ba7-9e27-d630aaefbf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expert(obs, env, other):\n",
    "    if other is None:\n",
    "        state = env.env.initial_state\n",
    "    else:\n",
    "        state = other[\"orig_state\"]\n",
    "    if state is None:\n",
    "        print(f'state is none')\n",
    "        return ACTIONS.index(\"WAIT\")\n",
    "    action = act(self, state)\n",
    "    if action is None:\n",
    "        return ACTIONS.index(\"WAIT\")\n",
    "    return ACTIONS.index(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48bf9438-9da8-466b-853c-4f6725ff72c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stable_baselines3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_41447/3302374476.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVecEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVecFrameStack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m def generate_expert_traj(model, save_path=None, env=None, n_timesteps=0,\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines3'"
     ]
    }
   ],
   "source": [
    "# Adapted for Bombergym from\n",
    "# https://github.com/Stable-Baselines-Team/stable-baselines/blob/master/stable_baselines/gail/dataset/record_expert.py\n",
    "import os\n",
    "import warnings\n",
    "from typing import Dict\n",
    "\n",
    "import cv2  # pytype:disable=import-error\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "from stable_baselines3.common.vec_env import VecEnv, VecFrameStack\n",
    "\n",
    "def generate_expert_traj(model, save_path=None, env=None, n_timesteps=0,\n",
    "                         n_episodes=100, image_folder='recorded_images'):\n",
    "    \"\"\"\n",
    "    Train expert controller (if needed) and record expert trajectories.\n",
    "    .. note::\n",
    "        only Box and Discrete spaces are supported for now.\n",
    "    :param model: (RL model or callable) The expert model, if it needs to be trained,\n",
    "        then you need to pass ``n_timesteps > 0``.\n",
    "    :param save_path: (str) Path without the extension where the expert dataset will be saved\n",
    "        (ex: 'expert_cartpole' -> creates 'expert_cartpole.npz').\n",
    "        If not specified, it will not save, and just return the generated expert trajectories.\n",
    "        This parameter must be specified for image-based environments.\n",
    "    :param env: (gym.Env) The environment, if not defined then it tries to use the model\n",
    "        environment.\n",
    "    :param n_timesteps: (int) Number of training timesteps\n",
    "    :param n_episodes: (int) Number of trajectories (episodes) to record\n",
    "    :param image_folder: (str) When using images, folder that will be used to record images.\n",
    "    :return: (dict) the generated expert trajectories.\n",
    "    \"\"\"\n",
    "    assert env is not None, \"You must set the env in the model or pass it to the function.\"\n",
    "\n",
    "    assert (isinstance(env.observation_space, spaces.Box) or\n",
    "            isinstance(env.observation_space, spaces.Discrete)), \"Observation space type not supported\"\n",
    "\n",
    "    assert (isinstance(env.action_space, spaces.Box) or\n",
    "            isinstance(env.action_space, spaces.Discrete)), \"Action space type not supported\"\n",
    "\n",
    "    actions = []\n",
    "    observations = []\n",
    "    rewards = []\n",
    "    episode_returns = np.zeros((n_episodes,))\n",
    "    episode_starts = []\n",
    "\n",
    "    ep_idx = 0\n",
    "    obs = env.reset()\n",
    "    episode_starts.append(True)\n",
    "    reward_sum = 0.0\n",
    "    idx = 0\n",
    "    # state and mask for recurrent policies\n",
    "    state, mask = None, None\n",
    "    other=None\n",
    "\n",
    "    while ep_idx < n_episodes:\n",
    "        observations.append(obs)\n",
    "        action = model(obs, env, other)\n",
    "\n",
    "        obs, reward, done, other = env.step(action)\n",
    "\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        episode_starts.append(done)\n",
    "        reward_sum += reward\n",
    "        idx += 1\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            # Reset the state in case of a recurrent policy\n",
    "            state = None\n",
    "\n",
    "            episode_returns[ep_idx] = reward_sum\n",
    "            reward_sum = 0.0\n",
    "            ep_idx += 1\n",
    "\n",
    "    observations = np.concatenate(observations).reshape((-1,) + env.observation_space.shape)\n",
    "    #elif isinstance(env.observation_space, spaces.Discrete):\n",
    "    #    observations = np.array(observations).reshape((-1, 1))\n",
    "    #elif record_images:\n",
    "    #    observations = np.array(observations)\n",
    "\n",
    "    if isinstance(env.action_space, spaces.Box):\n",
    "        actions = np.concatenate(actions).reshape((-1,) + env.action_space.shape)\n",
    "    elif isinstance(env.action_space, spaces.Discrete):\n",
    "        actions = np.array(actions).reshape((-1, 1))\n",
    "\n",
    "    rewards = np.array(rewards)\n",
    "    episode_starts = np.array(episode_starts[:-1])\n",
    "\n",
    "    assert len(observations) == len(actions)\n",
    "\n",
    "    numpy_dict = {\n",
    "        'actions': actions,\n",
    "        'obs': observations,\n",
    "        'rewards': rewards,\n",
    "        'episode_returns': episode_returns,\n",
    "        'episode_starts': episode_starts\n",
    "    }  # type: Dict[str, np.ndarray]\n",
    "\n",
    "    for key, val in numpy_dict.items():\n",
    "        print(key, val.shape)\n",
    "\n",
    "    if save_path is not None:\n",
    "        np.savez(save_path, **numpy_dict)\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return numpy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6573caac-a203-441b-897b-8230b8290d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No protocol specified\n",
      "/home/g556b/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/stable_baselines/__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions (22173, 1)\n",
      "obs (22173, 5, 17, 17)\n",
      "rewards (22173,)\n",
      "episode_returns (100,)\n",
      "episode_starts (22173,)\n",
      "Total trajectories: 1\n",
      "Total transitions: 465\n",
      "Average returns: 3.16\n",
      "Std for returns: 2.6710297639674483\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines import PPO2\n",
    "from stable_baselines.gail import ExpertDataset\n",
    "# Using only one expert trajectory\n",
    "# you can specify `traj_limitation=-1` for using the whole dataset\n",
    "dataset = ExpertDataset(expert_path='test-experttraj.npz',\n",
    "                        traj_limitation=1, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8875070c-c4ad-464c-a9fe-c860d9a113ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env in a DummyVecEnv.\n",
      "WARNING:tensorflow:From /home/g556b/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/g556b/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/g556b/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/g556b/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/g556b/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/g556b/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/g556b/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/g556b/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/g556b/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/g556b/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/g556b/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py:198: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/g556b/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/g556b/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/g556b/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py:240: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/g556b/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "Pretraining with Behavior Cloning...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (128, 1445) for Tensor 'input/Ob:0', which has shape '(?, 5, 17, 17)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_41695/3491538863.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MlpPolicy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Pretrain the PPO2 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# As an option, you can train the RL agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/stable_baselines/common/base_class.py\u001b[0m in \u001b[0;36mpretrain\u001b[0;34m(self, dataset, n_epochs, learning_rate, adam_epsilon, val_interval)\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0mactions_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexpert_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 }\n\u001b[0;32m--> 351\u001b[0;31m                 \u001b[0mtrain_loss_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_loss_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.12/envs/bomberman_rl_exp_oldpy/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1157\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (128, 1445) for Tensor 'input/Ob:0', which has shape '(?, 5, 17, 17)'"
     ]
    }
   ],
   "source": [
    "\n",
    "model = PPO2('MlpPolicy', env, verbose=1)\n",
    "# Pretrain the PPO2 model\n",
    "model.pretrain(dataset, n_epochs=1000)\n",
    "\n",
    "# As an option, you can train the RL agent\n",
    "# model.learn(int(1e5))\n",
    "\n",
    "# Test the pre-trained model\n",
    "env = model.get_env()\n",
    "obs = env.reset()\n",
    "\n",
    "reward_sum = 0.0\n",
    "for _ in range(1000):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "                print(reward_sum)\n",
    "                reward_sum = 0.0\n",
    "                obs = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ffb921-9a94-4b01-88b1-deb44a11e9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
